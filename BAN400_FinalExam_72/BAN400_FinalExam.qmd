---
title: "Ban400 home exam, fall 2023, Candidate no 72"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = F)
```

**Statement on referencing and generative AI:** I confirm that all the material in this exam, including any code and text generated by AI models, has been appropriately referenced and attributed in accordance with standard academic practices.

# Cherry-picking regressions

## Disclaimer

The code in this file was automatically formatted using the styler package.

## Libraries

In addition to the packages used in the lectures, please install combinat, stargazer, RcppArmadillo and plotly to be able to run this code.

```{r, eval=T, error=TRUE, message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(assertthat)
library(tictoc)
library(furrr)
library(foreach)
library(doParallel)

library(combinat)
library(stargazer)
library(RcppArmadillo)
library(plotly)
```

## Parallel Computing

I do not use parallel computing for the majority of the problems in this exam, since all code runs in a reasonable amount of time. I carried out tests whether some calculations were faster using the future_map function of the furrr package instead of map. However, it did not result in any speed gains except for the last simulation (for the extra analysis) in Problem 6. Consequently, this is the only problem where I used the future_map function. I suppose that all other Problems ran fast enough that the cost of initializing parallel computing outweighed the performance gains. Furthermore, I used the doParallel and foreach packages to speed up the for-loop in Problem 6, when estimating 100 models for each number of explanatory variables between 10 and 66 (basically an embarrassingly parallel problem). I tried to write the while-loop of the est_random_models-function in Problem 6 in parallel, but did not manage to do so with any speed gains, while still keeping it in a function. Since the convenience of having a function that estimates random models is important for the rest of my solution in Problem 6, I decided not to write this specific part in parallel. It does not come at any advantage, since the function provides the possibility of setting a timeout or maximum number of models, and "suitable" models were usually found within a reasonable amount of time. 

## Problem 1

```{r, eval=T, error=TRUE, message=TRUE, warning=FALSE}
# read data and convert to numeric
bacedata <- read_csv("BACEDATA.csv", skip = 2, show_col_types = FALSE) %>%
  select(-OBS, -CODE, -COUNTRY) %>%
  map_df(., as.numeric)
```

When running the code, there are warning messages. I set the warning=FALSE option in this code chunk for better readability. The warning message indicates that in the conversion of the datatypes, NAs are produced. To make sure that this is not a programming bug but actual missing values in the .csv-file, I inspect the raw data before datatype conversion further. As below code demonstrates, all values that produced NAs during type conversion are dots (".") in the original datafile.

```{r, eval=T, error=TRUE, message=TRUE, warning=FALSE}
# function to return distinct values from original data that cause NAs during conversion
NA_produced_check <- function(column) {
  converted <- as.numeric(column)

  col_vector <- as.vector(column)

  NA_origins <- col_vector[is.na(converted)] %>% unique()

  return(ifelse(length(NA_origins) == 0, "No NAs in Column", NA_origins))
}

# check for values that cause NAs during conversion
NA_originated_values <- read_csv("BACEDATA.csv", skip = 2, show_col_types = FALSE) %>%
  select(-OBS, -CODE, -COUNTRY) %>%
  map_chr(., NA_produced_check) %>%
  unique()

# display distinct values that cause NAs during conversion
# we see it's only dots, so we do not have to worry about them now
print(paste0(
  "The following values in the original data cause NAs: '",
  NA_originated_values[NA_originated_values != "No NAs in Column"], "'"
))
```
For now, I do not worry about the NAs. However, we must keep in mind that we have incomplete cases in our data and possibly consider this for further assigments, when using functions that do not automatically truncate NAs where necessary.

## Problem 2

First, I create a function that takes as input a data frame (tibble) containing the data we want to estimate the models on, and a vector containing all explanatory variables that we want to include in the model, excluding our variable of interest "SOCIALIST". Since we want to form different combinations of certain variables for different regression models, I also specify which variables we want to "draw" the models from (explanatory_var).

```{r, eval=T, error=TRUE, message=TRUE}
# function to estimate model, takes dataframe and vector of explanatory variables
reg_fraud <- function(df, v) {
  df %>%
    select(GR6096, SOCIALIST, which(names(df) %in% v)) %>%
    lm(GR6096 ~ ., data = .)
}

# first, create vector with explanatory controls of interest
explanatory_var <- c(
  "LANDAREA",
  "PRIEXP70",
  "EAST",
  "AVELF",
  "ORTH00",
  "POP6560",
  "REVCOUP",
  "OTHFRAC",
  "MINING",
  "PROT00"
)
```

Next, we draw all possible combinations of variables from the explanatory_var vector. I do this by using the "combinat"-package. I found this package and the corresponding code snippet on [this page](https://www.statisticsglobe.com/get-all-possible-subsets-vector-r) and slightly modified the whole pipeline to fit this question. Most importantly, we must not forget to include one model with only "SOCIALIST" as an explanatory variable, excluding any controls. All combinations of explanatory variables (excluding SOCIALIST) are stored as vectors in the $explanatory_comb-column in the explanatory_combs data frame. This makes it easy to iterate over these possible variable combinations, and construct and estimate the respective models for each row using the function specified above. This will also come in very handy for further questions, where we can just rely on the data frame created in this section. The estimated model is also stored in one column of this data frame (model_A2, to distinguish it from the models in the next sections). To extract the coefficient estimate of each model, I write the function "get_coef_SOCIALIST". This function is then used in the pipeline for the histogram of coefficient estimates. To display a more precise histogram, I overwrite the default 30 bins with 50 bins. Finally, because a basic histogram would be very boring, I convert the ggplot2 object to a plotly object, making it interactive. [This](https://r-graph-gallery.com/interactive-charts.html) is a cool page to find inspiration for interactive plots, where I got the idea from.

```{r, eval=T, error=TRUE, message=TRUE}
# generate all possible subsets of this vector
# do not forget the possibility to use only SOCIALIST, without controls
explanatory_combs <- lapply(1:length(explanatory_var),
  combinat::combn,
  x = explanatory_var,
  simplify = F
) %>%
  unlist(., recursive = F) %>%
  as.matrix() %>%
  as_tibble(.name_repair = "unique") %>%
  rename(explanatory_comb = ...1) %>%
  rbind("")

# now estimate the model for every combination generated above
# I separate this from the pipe above for documentation purposes
explanatory_combs$model_A2 <- map(explanatory_combs$explanatory_comb,
  reg_fraud,
  df = bacedata
)

# function to extract the coefficient on SOCIALIST in each model
get_coef_SOCIALIST <- function(model) {
  model[1]$coefficients[names(model[1]$coefficients) == "SOCIALIST"]
}

# plot coefficients on SOCIALIST for all models
coef_hist <- explanatory_combs %>%
  mutate(coef_SOCIALIST = map_dbl(model_A2, get_coef_SOCIALIST)) %>%
  ggplot(aes(x = coef_SOCIALIST)) +
  geom_histogram(bins = 50) +
  geom_vline(
    xintercept = median(map_dbl(explanatory_combs$model_A2, get_coef_SOCIALIST)),
    color = "red",
    linewidth = 1.5
  ) +
  annotate(
    "text",
    x = -0.009, y = 60,
    label = paste0("median ~ ", round(median(map_dbl(explanatory_combs$model_A2, get_coef_SOCIALIST)), 6)),
    vjust = 1.1, hjust = 3, color = "red", size = 6
  ) +
  labs(
    title = "Distribution of coefficient on SOCIALIST",
    x = "Coefficient on SOCIALIST"
  ) +
  theme_minimal()

ggplotly(coef_hist)
```

The histogram shows that most models have a coefficient on SOCIALIST between around -0.01 and -0.0025. All coefficients are negative with a median or around -0.0064.

## Problem 3

Fortunately, the lm-function can take a model specification as string for the formula-input. I thus create this "model call" in the first place. Since the tests for the function put the variable of interest "SOCIALIST" in the last place, it is crucial to have variable_to_assess in the last argument of the paste-function. After estimating the model, coefficient and p-value are easily extracted from the summary-function.

```{r, eval=T, error=TRUE, message=TRUE}
is_variable_significant <- function(covariates, response, variable_to_assess, data = bacedata) {
  # create model as string
  model_call <- paste0(
    response, " ~ ",
    paste0(covariates, collapse = " + "), " + ",
    variable_to_assess
  )

  # estimate model
  model_est <- lm(model_call, data = data)

  # extract coefficients and p value of response from model
  coefficient_estimate <- summary(model_est)$coefficients[variable_to_assess, "Estimate"]
  coefficient_pvalue <- summary(model_est)$coefficients[variable_to_assess, "Pr(>|t|)"]

  # return data frame of results
  tibble(
    model_call = model_call,
    assessed_variable = variable_to_assess,
    coefficient_estimate = coefficient_estimate,
    p_value = coefficient_pvalue
  )
}
```

As we can see, all test are successfully passed:

```{r, eval=T, error=TRUE, message=FALSE}
library(assertthat)

# Tests for assignment 3. Leave this code chunk *unchanged* and
# ensure you run the tests *after* the chunk with you answer to
# assignment 3.

library(assertthat)

tmp <-
  is_variable_significant(
    response = "GR6096",
    variable_to_assess = "SOCIALIST",
    covariates = c("COLONY", "CONFUC")
  )

assert_that(tmp$model_call == "GR6096 ~ COLONY + CONFUC + SOCIALIST",
  msg = "Regression specification is not correct"
)
assert_that(tmp$assessed_variable == "SOCIALIST",
  msg = "Assess variable is not correct"
)
assert_that(round(tmp$coefficient_estimate, 8) == -0.01305317,
  msg = "The coefficient estimate of SOCIALIST is not correct"
)
assert_that(round(tmp$p_value, 9) == 0.004308533,
  msg = "The P-value of SOCIALIST is not correct"
)

rm(tmp)
```

## Problem 4

Using the function is_variable_significant from A3, we can re-estimate all models from A2. I use the map function to estimate every model with this function and store the resulting 1-row-dataframe in another column in the explanatory_combs-dataframe, model_A4. Filtering for positive coefficients on SOCIALIST, that are significant at the 5%-level, yields 0 models.

```{r, eval=T, error=TRUE, message=TRUE}
# start timer
tic()

# add a column with estimated models using function from A3 to explanatory_combs df
explanatory_combs$model_A4 <- map(explanatory_combs$explanatory_comb,
  is_variable_significant,
  response = "GR6096",
  variable_to_assess = "SOCIALIST",
  data = bacedata
)

# report elapsed time
toc()

# filter for significant positive coefficients
explanatory_combs %>%
  unnest(model_A4) %>%
  filter(coefficient_estimate > 0, p_value < 0.05)
```

It is a little bit boring to just say "we have 0 positive significant coefficients". A nice visual representation of estimates and corresponding p-values would look much more satisfying. The next code chunk produces a nice scatterplot, plotting the p-values on coefficient-estimates. The red line indicates the 0-threshold for the coefficient and the blue line indicates the 0.05-threshold for the corresponding p-value. As this plot clearly demonstrates, no coefficient estimates on SOCIALIST are positive in all 1024 estimated models and comparably few fall below the 5%-threshold to reject the null-hypothesis that the estimate is different from 0.

```{r, eval=T, error=TRUE, message=TRUE}
# plot estimates and pvalues
coef_pval_plot <- explanatory_combs %>%
  unnest(model_A4) %>%
  ggplot(aes(x = coefficient_estimate, y = p_value)) +
  geom_point() +
  geom_vline(xintercept = 0, color = "red", linewidth = 1.5) +
  geom_hline(yintercept = 0.05, color = "blue", linewidth = 1.5) +
  labs(
    title = "P-values of coefficient estimates",
    x = "Coefficient estimate",
    y = "p-value"
  ) +
  theme_minimal() +
  annotate("text", x = -0.002, y = 0.6, label = "Coefficient = 0", vjust = 1.1, hjust = 3, color = "red", size = 6) +
  annotate("text", x = -0.002, y = 0.08, label = "p = 0.05", hjust = 1.5, vjust = -0.5, color = "blue", size = 6)

# make interactive
ggplotly(coef_pval_plot)
```

## Problem 5

Since the goal of A5 is to improve performance of the is_variable_significant function, I will philosophize a little bit on potential pitfalls when evaluating performance, as i encountered them when solving this assignment. Because we have mainly used dplyr in reshaping and manipulating datasets in this course and in others, I started to solve this assignment using dplyr. Why do I need dplyr after all in this, when I did not need it to define the function in A3? The answer is that RcppArmadillo:fastLmPure needs different input than the lm-function. If we fail to provide this input, as described in the [documentation](https://search.r-project.org/CRAN/refmans/RcppArmadillo/html/fastLm.html) for this package, the function will either fail or not be able to exploit the speed gains. Furthermore, fastLmPure does not handle NAs automatically (lm does). Bottom line: We need to reshape and filter the dataset **every time the function is called**. We need to create a matrix with columns for all explanatory variables (including var of interest) **plus** one extra column containing only 1's for the intercept. This constitutes the X-input for fastLmPure. For the y-input, we need a single atomic vector containing all corresponding values of the response variable.

### 1) dplyr version of the function

The next code chunk demonstrates how I first tried to solve this with dplyr. As we can see, it takes **much** longer (on my computer around 3 times as long) to evaluate all models from A2. Note how data_filtered is created and how X and y are defined: dplyr functions.

```{r, eval=T, error=TRUE, message=TRUE}
# using dplyr instead of base R to reshape the data to fit the
# input-parameters needed for fastLmPure funtion
is_variable_significant_cpp_dplyr <- function(covariates, response, variable_to_assess, data = bacedata) {
  # create model as string
  model_call <- paste0(
    response, " ~ ",
    paste0(covariates, collapse = " + "), " + ",
    variable_to_assess
  )

  # for fastLmPure to estimate using c++, we need to reshape the input
  # fastLmPure cannot handle NAs
  data_filtered <- data %>%
    select(all_of(response), which(names(data) %in% covariates), all_of(variable_to_assess)) %>%
    filter(complete.cases(.))

  # fastLmPure needs matrix input for explanatories
  X <- cbind(1, as.matrix(data_filtered %>% select(-all_of(response))))

  # y must be single atomic vector containing all values for response variable
  y <- data_filtered %>% pull(response)

  # now, using X and y, we can estimate model with fastLmPure
  model_est <- RcppArmadillo::fastLmPure(y = y, X = X)

  # index to extract coefficient etc.
  index <- ifelse(any(covariates == ""), 0, length(covariates)) + 2

  # extract the coefficient estimate from the model
  coefficient_estimate <- model_est$coefficients[index, ]

  # calculate the p-value from the variable to assess
  coefficient_pvalue <- 2 * pt(
    abs(model_est$coefficients[index, ] / model_est$stderr[index, ]),
    model_est$df.residual,
    lower.tail = FALSE
  )

  # return data frame of results
  tibble(
    model_call = model_call,
    assessed_variable = variable_to_assess,
    coefficient_estimate = coefficient_estimate,
    p_value = coefficient_pvalue
  )
}


# start timer
tic()

# add a column with estimated models using function from A3 to explanatory_combs df
explanatory_combs$model_A5 <- map(explanatory_combs$explanatory_comb,
  is_variable_significant_cpp_dplyr,
  response = "GR6096",
  variable_to_assess = "SOCIALIST",
  data = bacedata
)

# report elapsed time
toc()
```

### 2) Base R version of the function

Knowing that dplyr, although very convenient, in many cases is much slower than base R (see for example [this discussion](https://stackoverflow.com/questions/54324620/why-is-dplyr-so-slow)), I tried to rewrite the function using base R. The code chunk below demonstrates this. Now the function does not only run faster than the dplyr version, but also faster than the function from A3, and we can actually compare the speed of the functions, because the overhead is reduced to a minimum and most of the time is (probably) spent on the model evaluation (this is a guess, I did not test this.).

```{r, eval=T, error=TRUE, message=TRUE}
is_variable_significant_cpp <- function(covariates, response, variable_to_assess, data = bacedata) {
  # create model as string
  model_call <- paste0(
    response, " ~ ",
    paste0(covariates, collapse = " + "), " + ",
    variable_to_assess
  )

  # all explantory variables, including variable of interest
  # must be filtered to != "" because one model contains only var of interes
  explanatory_var <- c(covariates, variable_to_assess)
  explanatory_var <- explanatory_var[explanatory_var != ""]

  # all variables, including response variable
  all_var <- c(response, explanatory_var)

  # data must be filtered: fastLmPure cannot handle NAs
  data_filtered <- na.omit(data[all_var])

  # define input parameters for fastLmPure
  # matrix of all explanatory values and vector of response values
  X <- as.matrix(cbind(1, data_filtered[explanatory_var]))
  y <- as.vector(data_filtered[[response]])

  # Fit the model using fastLmPure and input parameters defined above
  model_est <- RcppArmadillo::fastLmPure(y = y, X = X)

  # Calculate index for the coefficient
  # +2, because one intercept, and the last one is the var of interest
  index <- (ifelse(any(covariates == ""), 0, length(covariates)) + 2)

  # Extract coefficient and standard error needed to compute t statistic
  coefficient_estimate <- model_est$coefficients[index, ]
  coefficient_stderr <- model_est$stderr[index, ]

  # calculate degrees of freedom (n - k - 1) with k exlanatories and n observations
  degrees_of_freedom <- length(y) - length(explanatory_var) - 1

  # Calculate p-value of two-sided t-test
  coefficient_pvalue <- 2 * pt(abs(coefficient_estimate / coefficient_stderr),
    degrees_of_freedom,
    lower.tail = FALSE
  )

  # return data frame of results
  tibble(
    model_call = model_call,
    assessed_variable = variable_to_assess,
    coefficient_estimate = coefficient_estimate,
    p_value = coefficient_pvalue,
    stderr = coefficient_stderr
  )
}

# start timer
tic()

# add a column with estimated models using function from A3 to explanatory_combs df
explanatory_combs$model_A5 <- map(explanatory_combs$explanatory_comb,
  is_variable_significant_cpp,
  response = "GR6096",
  variable_to_assess = "SOCIALIST",
  data = bacedata
)

# report elapsed time
toc()
```

Note that the function also returns the standard error (in addition to what the question asks for). I did this to do some testing on why the p-values are sometimes different. It does not affect the answer to this question or the tests to pass.

In the end, speed gains of around 30% (on my computer) for estimating 1000 models might not seem like a significant improvement. When we estimate models for 100 seconds, however, this will provide a significant improvement.

The main difference in the two implementations is, that the fastLmPure-approach requires a lot more data-reshaping and filtering before we can actually estimate the model, since the required inputs are very different. Depending on the form of data we have and how we do the reshaping, this might in the end cause the whole function to be overall slower. However, with the right reshaping, we see that there are indeed significant speed gains of around 30% (on my computer). Another reason why fastLmPure is so fast is the relatively limited output compared to Lm. We only get the coefficient estimates, standard errors and degrees of freedom. Depending on the kind of statistical analysis we want to conduct on the models afterwards, this might cause serious problems because we would have to do many calculations after estimating the models. This would probably nullify the speed gains in the first step. However, in our case, we are happy with the just the estimates, and a t-statistic is easily calculated with the standard errors.

Now we can compare the results from the model in A4 with the results from the model we just estimated. As below code chunk shows, there are only very small deviations. The biggest deviation in coefficient estimates is *somenumber* to the power of -17, which is so small that we can attribute the deviations to rounding errors. For p-values, however, the deviations are bigger. For now I cannot explain why this happens in fastLmPure, but I will explore it further in Problem 6. Bottom line: the two functions output the exact same coefficients, but p-values are not necessarily exactly the same.

```{r, eval=T, error=TRUE, message=TRUE}
# create data frame with coefficients and pvalues from model A4 and A5
compare_models <- explanatory_combs %>%
  unnest(model_A4) %>%
  rename(
    coef_model_A4 = coefficient_estimate,
    pval_model_A4 = p_value
  ) %>%
  select(explanatory_comb, coef_model_A4, pval_model_A4, model_A5) %>%
  unnest(model_A5) %>%
  rename(
    coef_model_A5 = coefficient_estimate,
    pval_model_A5 = p_value
  ) %>%
  select(explanatory_comb, coef_model_A4, pval_model_A4, coef_model_A5, pval_model_A5) %>%
  mutate(
    coef_diff = coef_model_A5 - coef_model_A4,
    pval_diff = pval_model_A5 - pval_model_A4
  )

# deviations in coefficients between model A5 and A4
compare_models %>%
  select(coef_diff) %>%
  arrange(desc(abs(coef_diff))) %>%
  head(5)

# deviations in pvalues between model A5 and A4
compare_models %>%
  select(pval_diff) %>%
  arrange(desc(abs(pval_diff))) %>%
  head(5)
```

All tests are successfully passed:

```{r, eval=T, error=TRUE, message=FALSE}
library(assertthat)

# Tests for assignment 5. Leave this code chunk *unchanged* and
# ensure you run the tests *after* the chunk with you answer to
# assignment 5.

library(assertthat)

tmp <-
  is_variable_significant_cpp(
    response = "GR6096",
    variable_to_assess = "SOCIALIST",
    covariates = c("COLONY", "CONFUC")
  )

assert_that(tmp$model_call == "GR6096 ~ COLONY + CONFUC + SOCIALIST",
  msg = "Regression specification is not correct"
)
assert_that(tmp$assessed_variable == "SOCIALIST",
  msg = "Assess variable is not correct"
)
assert_that(round(tmp$coefficient_estimate, 8) == -0.01305317,
  msg = "The coefficient estimate of SOCIALIST is not correct"
)
assert_that(round(tmp$p_value, 9) == 0.004308533,
  msg = "The P-value of SOCIALIST is not correct"
)

rm(tmp)
```

## Problem 6

For problem 6, I chose to write a random search algorithm. I specified a function for this algorithm so I can conveniently call this function at a later point, which will come in very handy for a special analysis I want to do for a problem I encountered when solving this task.

The function est_random_models takes the following input parameters (= [value] indicates default):

- **variables:** a single atomic vector containing all (explanatory) variables that should be considered in the random model
search
- **coef_th = 0:** coefficient threshold. The function will stop searching when a model with a significant coefficient > coef_th is found
- **pval_th = 0.05:** p-value threshold. The function will stop searching when a positive coefficient with p-value < pval_th is found.
- **n_var = c(0:15):** single atomic number with number of variables. From this sequence, a random number is drawn as the number of variables.
- **n_mod = 100000:** the function will estimate exactly n_mod models.
- **setseed = FALSE:** set seed for reproducability.

It will make sense through the rest of this Problem, why I need all these parameters in the function.

```{r, eval=T, error=TRUE, message=TRUE}
# vector containing all optional explanatory variables that can be combinated
expl_all <- names(bacedata)[!names(bacedata) %in% c("GR6096", "SOCIALIST")]

# random approach. Create function to conveniently call later
est_random_models <- function(variables = expl_all,
                              coef_th = 0,
                              pval_th = 0.05,
                              n_var = c(0:15),
                              n_mod = 100000, timeout = 30,
                              setseed = FALSE) {
  # set starting values for coef and pval and i (n models)
  coef <- 0
  pval <- 1
  i <- 0

  # create tibble with all models that are assessed. We want to keep track of them
  results <- tibble(
    expl_var = list(),
    results = data.frame()
  )

  # set start time with sys.time()
  start_time <- Sys.time()

  # set seed for reproducability, important to get the same "fraud model"
  # however, for my extra analysis, I do not want to set a seed
  if (setseed == TRUE) {
    set.seed(123)
  }

  # randomly estimate models for a certain amount of time
  while ((coef <= coef_th | pval >= pval_th) && difftime(Sys.time(), start_time, units = "secs") < timeout && i < n_mod) {
    # generate a random number between 0 and n_var (parameter for function)
    n_expl <- sample(n_var, size = 1)

    # now get a random sample with n_expl explanatory variables from the expl_var vector
    expl_incl <- sample(variables, size = n_expl)

    # estimate the model with the function from A5
    model_tmp <- is_variable_significant_cpp(
      response = "GR6096",
      variable_to_assess = "SOCIALIST",
      covariates = expl_incl
    )

    # reset the coefficient and pvalue and i (n_mod)
    coef <- model_tmp$coefficient_estimate
    pval <- model_tmp$p_value
    i <- i + 1

    # append row to tibble where we keep track of all tries
    results <- results %>%
      add_row(
        expl_var = list(expl_incl),
        results = model_tmp
      )
  }

  # print message if model is found
  if (coef > 0 && pval < 0.05) {
    print(paste0("Estimated ", as.character(nrow(results)), " random models and found one significant positive estimate."))
  }

  results
}
```

When I initially wrote the function est_random_models, I did not restrict the number of explanatory variables. So theoretically, there could have been up to 66 explanatories + SOCIALIST in the model. Also, a seemingly significant positive estimate on SOCIALIST was always found very quickly after estimating only very few models. When I then estimated this model with Lm(), however, the coefficient was not significant anymore. In other words, p-value of Lm and fastLmPure differed, sometimes by a great difference. This was always the case when the number of variables was relatively large (for example 50). Digging a little bit deeper, I saw that the standard errors (needed for p-value calculation) were completely different in the lm-model versus the fastLmPure-model. I researched a lot about this and also consulted chatGPT4 for why this could be the case. However, I did not find an explanation.

I know that fitting a model with 50 explanatory variables to only around 90 observations (after NAs removed), does not make a lot of sense from a statistical perspective. Still, it would have been nice to know why Lm produces different results than fastLmPure in these cases (estimates were the same). However, I woke my inner statistician and made the model choice to restrict the number of variables be drawn from a random number between 0 and 15 (default value for n_var in est_random_models()). The following code chunks search for a model with significant positive coefficient on SOCIALIST, estimate this model with Lm, and display the results in a regression table.

```{r, eval=T, error=TRUE, message=TRUE}
# get the fraud model results from the fastLmPure estimation
fraud_results_fastLmPure <- est_random_models(n_var = 15, timeout = 100, setseed = TRUE) %>%
  unnest(results) %>%
  filter(coefficient_estimate > 0, p_value < 0.05)

# estimate this model with the lm function
fraud_model_lm <- fraud_results_fastLmPure %>%
  filter(coefficient_estimate > 0, p_value < 0.05) %>%
  pull(model_call) %>%
  lm(., data = bacedata)

# display regression results in table with stargazer package
stargazer(fraud_model_lm,
  type = "text",
  title = "Regression Results of Fraud Model",
  label = "tab:regression_results",
  header = FALSE,
  model.names = FALSE,
  dep.var.labels.include = FALSE
)
```

The credibility of these results are obviously very questionable. In this assignment, we systematically searched for any model that seems to discover a significant positive effect of SOCIALIST on GDP growth rate. To argue for credibility of the model, however, we need to argue for a causal effect. As studied in ECN402, we need to argue for the Gauss-Markov assumptions, particularly the zero conditional mean assumption. I am not going to assess the model for any violations of the GM-assumptions, since this clearly exceeds the scope of BAN400. However, we must know that there is more for an econometric model to be able to convey a causal effect of a variable on another. Furthermore, the econometrician plays an important role in specifying the model, while often only detailed domain knowledge can be used to argue for inclusion or exclusion of variables. With our many-models approach, we can clearly not produce results that incorporate any specific domain knowledge. Concluding, without commenting on the specific coefficients and standard erros, the results are clearly not credible.

Just checking back if the models are identical (or at least almost identical). Looks great!:

```{r, eval=T, error=TRUE, message=TRUE}
# check if coefficient and p-value are the same in lm vs fastLmPure
print(paste0(
  "coefficient Lm estimation: ",
  summary(fraud_model_lm)$coefficients["SOCIALIST", "Estimate"]
))
print(paste0(
  "coefficient fastLmPure estimation: ",
  fraud_results_fastLmPure$coefficient_estimate
))
print(paste0(
  "p-value Lm estimation: ",
  summary(fraud_model_lm)$coefficients["SOCIALIST", "Pr(>|t|)"]
))
print(paste0(
  "p-value fastLmPure estimation: ",
  fraud_results_fastLmPure$p_value
))
```
But back to the problem with different standard errors when estimating with Lm versus fastLmPure. This is still annoying. It would be at least a little bit more satisfying if I knew there was some systematic relationship between the number of variables and the difference in p-values between the two models. Why not simulate a few models for each n_var with fastLmPure, re-estimate the same with Lm and plot the difference in p-values on the number of covariates? Let us simulate 100 models per number of covariates.

We can set the coef threshold and the pval threshold to unrealistic values, so the function est_random_models does not stop when a positive significant is found (only after n_mod models):

```{r, eval=T, error=TRUE, message=TRUE}
# Register parallel backend to use multiple cores
numCores <- detectCores()
registerDoParallel(cores = numCores)

# iterate over n_var in parallel, estimate models for each n_var
random_model_sim <-
  foreach(
    i = 10:66,
    .combine = rbind,
    .packages = c("tidyverse"),
    # create a tibble that is returned after the parallel processing
    .init = tibble(
      n_var = numeric(),
      expl_var = list(),
      model_call = character(),
      coef_fastLmPure = numeric(),
      pval_fastLmPure = numeric(),
      stderr_fastLmPure = numeric()
    )
  ) %dopar% {
    # store all models for each n_var in a temporary object
    tmp_res <- est_random_models(
      coef_th = 100,
      pval_th = -1,
      n_var = c(i:i),
      n_mod = 100,
      timeout = 100
    ) %>%
      unnest(results)

    # return the tibble with all 5700 models
    tibble(
      n_var = i,
      expl_var = tmp_res$expl_var,
      model_call = tmp_res$model_call,
      coef_fastLmPure = tmp_res$coefficient_estimate,
      pval_fastLmPure = tmp_res$p_value,
      stderr_fastLmPure = tmp_res$stderr
    )
  }

# Stop the parallel backend
stopImplicitCluster()

# plan multisession for parallel use of the map function (future map)
plan(multisession, workers = detectCores())

# estimate the same models with the lm function
random_model_sim$model_lm <-
  future_map(random_model_sim$expl_var,
    is_variable_significant,
    response = "GR6096",
    variable_to_assess = "SOCIALIST",
    data = bacedata
  )

# back to sequential processing
plan(sequential)
```

Next, I create the plot and convert it to an interactive plot because that is cool. And wow! Look at this beautiful plot, perfectly supporting my assumption that higher numbers of covariates in a model produce bigger differences between Lm and fastLmPure (on average)! So satisfying. Furthermore, the plot shows that there is a systematic downward bias in the fastLmPure p-values, since all differences are either 0 or negative. How cool is that?! I still do not know from a statistical perspective, why fastLmPure produces these results, but this plot lets me sleep well!

The plot is generated with 100 random models per number of covariates between 10 and 66, so 5700 models in total.

```{r, eval=T, error=TRUE, message=TRUE}
# create plot to show how difference increases with n_var
random_model_sim_plot <- random_model_sim %>%
  select(n_var, model_lm, pval_fastLmPure) %>%
  unnest(model_lm) %>%
  rename(pval_lm = p_value) %>%
  select(n_var, pval_fastLmPure, pval_lm) %>%
  mutate(pval_diff = pval_fastLmPure - pval_lm) %>%
  ggplot(aes(x = n_var, y = pval_diff)) +
  geom_point() +
  geom_smooth() +
  labs(
    title = "Difference in p-values from fastLmPure vs lm",
    x = "number of explanatory variables",
    y = "difference in p-values"
  ) +
  theme_minimal()

# make it look nice and interactive
ggplotly(random_model_sim_plot)
```

## Problem 7

The way I have approached this exam was to try and solve the problems with what I already know. Storing all models and model results in data frames gives me a clear goal of what I want the result to look like, since It is easy to think two-dimensional. That being said, all code is written by myself. The closest to copying is the snippet to create all combinations of the variables (A2, see [this link](https://statisticsglobe.com/get-all-possible-subsets-vector-r)). I have used chatGPT4 as a search engine for simple problems I encountered where I did not know the answer to anymore. Examples of what I asked chatGPT4 include:

- check if any column of a dataframe in R contains NAs
- extract p-values from Lm-model-object
- filter out rows that contain any NAs from a data frame in R
- check if vector contains any empty strings in R
- run loop in R for 100 seconds
- how to calculate the p-value in R from coefficient and standard error
- why does the subtitle not show in a ggplotly plot when it shows in a ggplot plot
- how to add the value of a vline or hline to a ggplot in R

Asking these kind of questions to generative AI like chatGPT4 generates quick answers to simple questions, whereas searching on stackoverflow for example would take much longer. However, I explicitly did not ask chatGPT4 to solve entire exam questions, since I had a clear idea of what I wanted the results to look like, and chatGPT4 often has a very different style of coding, which I often find confusing. Furthermore, I have experienced in previous exams and other courses in this semester that chatGPT4 often delivers poor code that does not work as intended.

On Problem 6 I prompted chatGPT4 heavily on why the difference between p-values from the Lm-model and the fastLmPure-model are sometimes different. However, even after 2 hours of research, neither chatGPT4 nor manual search yielded any explanations. I decided to live with this "phenomenon", as you have read in my solution of Problem 6.

#### Session info

Leave this part unchanged. The cell below prints which packages and versions were used for creating the html-file.

```{r, eval=T}
sessionInfo()
```
